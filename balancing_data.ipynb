{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df54df7-90e4-49b4-bd54-ebd616290766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from echr import *\n",
    "from classifier import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import LongformerTokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e29094-adba-41b0-ac92-70b793e9e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_split(train_df, test_df, name=\"Dataset\"):\n",
    "    total = len(train_df) + len(test_df)\n",
    "    test_pct = (len(test_df) / total) * 100 if total > 0 else 0\n",
    "    lbl_dst_test = (len(test_df[test_df['violation']==1]) / len(test_df)) * 100\n",
    "    lbl_dst_train = (len(train_df[train_df['violation']==1]) / len(train_df)) * 100\n",
    "    print(f\"ðŸ“Š {name} split summary:\")\n",
    "    print(f\"- Train size: {len(train_df)}\")\n",
    "    print(f\"- Test size:  {len(test_df)}\")\n",
    "    print(f\"- Test %:     {test_pct:.2f}% of total ({total} cases)\")\n",
    "    print(f\"- Test lbl:    {lbl_dst_test:.2f}% violation\")\n",
    "    print(f\"- Train lbl:    {lbl_dst_train:.2f}% violation\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b128c647-aa83-4bcb-bc4f-eced32f184e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'datasets/original/cases_04_2024.json'\n",
    "article = '6' \n",
    "part = 'facts'\n",
    "split_year = 2015 # year on which to split training/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b1d58-2b58-456f-a424-9a147178cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chamber_df = create_dataset(json_path, article, part, 'Chamber')\n",
    "grand_chamber_df = create_dataset(json_path, article, part, 'Grand Chamber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a630f-4bf6-43f4-bee0-53f63358768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grand Chamber\n",
    "grand_chamber_df_test = grand_chamber_df[grand_chamber_df['year'] >= split_year]\n",
    "grand_chamber_df_train = grand_chamber_df[grand_chamber_df['year'] < split_year]\n",
    "summarize_split(grand_chamber_df_train, grand_chamber_df_test, name=\"Grand Chamber\")\n",
    "\n",
    "#Chamber\n",
    "chamber_df_test = chamber_df[chamber_df['year'] >= split_year]\n",
    "chamber_df_train = chamber_df[chamber_df['year'] < split_year]\n",
    "summarize_split(chamber_df_train, chamber_df_test, name=\"Chamber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a8ab5d-f318-40ad-82ac-a38cb56cd5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>violation</th>\n",
       "      <th>sample_weight</th>\n",
       "      <th>judgment_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>001-153349</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>5.The applicants were born in 1961, 1964 and 1...</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT UNANIMOUSLY\\n1.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>001-153349</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>5.The applicants were born in 1961, 1964 and 1...</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT UNANIMOUSLY\\n1.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>001-164199</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>5.The applicant was born in 1939 and lives in ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\n1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>001-200866</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>6.Details of the applicants are set out in the...</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\nDE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>001-200866</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>6.Details of the applicants are set out in the...</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\nDE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>001-170633</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>5.The first applicant was born in 1933 and liv...</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\n.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>001-183120</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>5.The applicant was born in 1975. He is curren...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT UNANIMOUSLY,\\n1. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>001-202539</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>1.The applicant was born in 1937 and lives in ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\nDE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>001-157537</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>5.The applicant was born in 1949 and lives in ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\n1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>001-200351</td>\n",
       "      <td>Chamber</td>\n",
       "      <td>5.The applicant was born in 1960 and lives in ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\nDE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id          id     body  \\\n",
       "1           1  001-153349  Chamber   \n",
       "2           2  001-153349  Chamber   \n",
       "7           7  001-164199  Chamber   \n",
       "17         17  001-200866  Chamber   \n",
       "18         18  001-200866  Chamber   \n",
       "26         26  001-170633  Chamber   \n",
       "29         29  001-183120  Chamber   \n",
       "37         37  001-202539  Chamber   \n",
       "41         41  001-157537  Chamber   \n",
       "44         44  001-200351  Chamber   \n",
       "\n",
       "                                                 text  year  violation  \\\n",
       "1   5.The applicants were born in 1961, 1964 and 1...  2015          1   \n",
       "2   5.The applicants were born in 1961, 1964 and 1...  2015          1   \n",
       "7   5.The applicant was born in 1939 and lives in ...  2016          1   \n",
       "17  6.Details of the applicants are set out in the...  2020          1   \n",
       "18  6.Details of the applicants are set out in the...  2020          1   \n",
       "26  5.The first applicant was born in 1933 and liv...  2017          0   \n",
       "29  5.The applicant was born in 1975. He is curren...  2018          1   \n",
       "37  1.The applicant was born in 1937 and lives in ...  2020          1   \n",
       "41  5.The applicant was born in 1949 and lives in ...  2015          1   \n",
       "44  5.The applicant was born in 1960 and lives in ...  2020          1   \n",
       "\n",
       "    sample_weight                                      judgment_info  \n",
       "1             1.0  FOR THESE REASONS, THE COURT UNANIMOUSLY\\n1.  ...  \n",
       "2             1.0  FOR THESE REASONS, THE COURT UNANIMOUSLY\\n1.  ...  \n",
       "7             1.0  FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\n1....  \n",
       "17            1.0  FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\nDE...  \n",
       "18            1.0  FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\nDE...  \n",
       "26            1.0  FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\n.....  \n",
       "29            1.0  FOR THESE REASONS, THE COURT UNANIMOUSLY,\\n1. ...  \n",
       "37            1.0  FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\nDE...  \n",
       "41            1.0  FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\n1....  \n",
       "44            1.0  FOR THESE REASONS, THE COURT, UNANIMOUSLY,\\nDE...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chamber_df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd74cfa-d4ee-493a-82ea-e1721820db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance multiple_datasets\n",
    "balanced_sets_chamber = generate_balanced_subsets(chamber_df_train, n=7, random_seed=42)\n",
    "balanced_sets_grand_chamber = generate_balanced_subsets(grand_chamber_df_train, n=7, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd61a3a-c711-4975-85e7-1947fb154d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store csvs\n",
    "grand_chamber_df_test.to_csv('datasets/test_grand_chamber.csv', index=False)\n",
    "chamber_df_test.to_csv('datasets/test_chamber.csv', index=False)\n",
    "for idx, (c_df, gc_df) in enumerate(zip(balanced_sets_chamber, balanced_sets_grand_chamber)):\n",
    "    c_df.to_csv('datasets/train_chamber_'+str(idx)+'.csv', index=False)\n",
    "    gc_df.to_csv('datasets/train_grand_chamber_'+str(idx)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57b2cb-aca6-442b-800b-6f337bd6f6d0",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab19e702-82e0-4189-ada7-ebf1289ad4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = {\n",
    "    'test': {\n",
    "        'chamber': pd.read_csv('datasets/test_chamber.csv'),\n",
    "        'grand_chamber': pd.read_csv('datasets/test_grand_chamber.csv'),\n",
    "    },\n",
    "    'train': {\n",
    "        'chamber': {idx: pd.read_csv('datasets/train_chamber_'+str(idx)+'.csv') for idx in range(0,7)},    \n",
    "        'grand_chamber': {idx: pd.read_csv('datasets/train_grand_chamber_'+str(idx)+'.csv') for idx in range(0,7)},    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b4a28-4554-48e6-abc4-695656fa7f35",
   "metadata": {},
   "source": [
    "### Store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c75ac4a5-0cf1-4a62-9fda-4de0b6933ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_NAME = 'allenai/longformer-base-4096'\n",
    "\n",
    "# Load Longformer model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Helper: Get mean pooled embedding for a single text\n",
    "@torch.no_grad()\n",
    "def get_longformer_embedding(text):\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    output = model(**tokens).last_hidden_state  # shape: [1, seq_len, hidden_size]\n",
    "    attention_mask = tokens['attention_mask'].unsqueeze(-1)  # [1, seq_len, 1]\n",
    "    masked_output = output * attention_mask  # Zero out paddings\n",
    "    summed = masked_output.sum(dim=1)\n",
    "    count = attention_mask.sum(dim=1).clamp(min=1e-9)\n",
    "    mean_pooled = summed / count  # [1, hidden_size]\n",
    "    \n",
    "    return mean_pooled.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Process and store embeddings\n",
    "def compute_and_store_chamber_embeddings(all_datasets, output_dir='datasets/embeddings/'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    tqdm.pandas()\n",
    "\n",
    "    for body in all_datasets['train'].keys():\n",
    "        for split_id, df in all_datasets['train'][body].items():\n",
    "            print(f\"Processing {body} split {split_id}...\")\n",
    "            \n",
    "            # Calculate embeddings\n",
    "            df = df.copy()\n",
    "            df['embedding'] = df['text'].progress_apply(get_longformer_embedding)\n",
    "            \n",
    "            # Save as pickle to preserve vectors\n",
    "            df.to_pickle(os.path.join(output_dir, f'{body}_split_{split_id}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ac23119-db3c-4e00-bf71-b6d4bed14abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chamber split 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [02:20<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chamber split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [02:22<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chamber split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [02:27<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chamber split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [02:28<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chamber split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [02:28<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chamber split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [02:31<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chamber split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [02:32<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grand_chamber split 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:14<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grand_chamber split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:14<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grand_chamber split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:14<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grand_chamber split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:14<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grand_chamber split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:14<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grand_chamber split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:14<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grand_chamber split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:14<00:00,  7.91it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_and_store_chamber_embeddings(all_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d5d4c-0d71-477a-9c8f-28b01126c8ce",
   "metadata": {},
   "source": [
    "### Calculate and store propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6451ef58-0926-42d5-9187-fe418f746e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_pickled_df(split_id, kind, folder='datasets/embeddings'):\n",
    "    path = os.path.join(folder, f\"{kind}_split_{split_id}.pkl\")\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def train_propensity_model(reference_df, target_df, score_on='target'):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model to distinguish between reference_df (label=0)\n",
    "    and target_df (label=1). Returns the propensity scores for the dataset specified\n",
    "    in 'score_on' ('reference' or 'target').\n",
    "    \"\"\"\n",
    "    reference_df = reference_df.copy()\n",
    "    target_df = target_df.copy()\n",
    "    reference_df['target'] = 0\n",
    "    target_df['target'] = 1\n",
    "\n",
    "    combined_df = pd.concat([reference_df, target_df], ignore_index=True)\n",
    "    X = np.vstack(combined_df['embedding'].values)\n",
    "    y = combined_df['target'].values\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "    ])\n",
    "    model.fit(X, y)\n",
    "\n",
    "    if score_on == 'reference':\n",
    "        X_score = np.vstack(reference_df['embedding'].values)\n",
    "    elif score_on == 'target':\n",
    "        X_score = np.vstack(target_df['embedding'].values)\n",
    "    else:\n",
    "        raise ValueError(\"score_on must be either 'reference' or 'target'\")\n",
    "\n",
    "    propensities = model.predict_proba(X_score)[:, 1]\n",
    "    return propensities\n",
    "\n",
    "def compute_and_save_propensities(split_id, input_folder='datasets/embeddings', output_folder='datasets/propensities'):\n",
    "    \"\"\"\n",
    "    Computes propensity scores of grand chamber cases (target) based on chamber cases (reference).\n",
    "    \"\"\"\n",
    "    # Load pickled dataframes with embeddings\n",
    "    chamber_df = load_pickled_df(split_id, 'chamber', input_folder)\n",
    "    grand_chamber_df = load_pickled_df(split_id, 'grand_chamber', input_folder)\n",
    "\n",
    "    # Compute propensities for grand chamber cases\n",
    "    propensities = train_propensity_model(chamber_df, grand_chamber_df, score_on='target')\n",
    "    grand_chamber_df['propensity_score'] = propensities\n",
    "\n",
    "    # Compute inverse propensity weights\n",
    "    grand_chamber_df['sample_weight'] = 1.0 / grand_chamber_df['propensity_score']\n",
    "\n",
    "    # Replace infinite/very large weights with cap\n",
    "    grand_chamber_df['sample_weight'] = grand_chamber_df['sample_weight'].clip(upper=20.0)\n",
    "\n",
    "    # Save updated grand_chamber_df\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    out_path = os.path.join(output_folder, f'grand_chamber_with_propensity_split_{split_id}.pkl')\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(grand_chamber_df, f)\n",
    "    print(f\"âœ… Stored: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f25df0c-0e6e-4122-94a9-75619781409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stored: datasets/balanced/propensities\\grand_chamber_with_propensity_split_0.pkl\n",
      "âœ… Stored: datasets/balanced/propensities\\grand_chamber_with_propensity_split_1.pkl\n",
      "âœ… Stored: datasets/balanced/propensities\\grand_chamber_with_propensity_split_2.pkl\n",
      "âœ… Stored: datasets/balanced/propensities\\grand_chamber_with_propensity_split_3.pkl\n",
      "âœ… Stored: datasets/balanced/propensities\\grand_chamber_with_propensity_split_4.pkl\n",
      "âœ… Stored: datasets/balanced/propensities\\grand_chamber_with_propensity_split_5.pkl\n",
      "âœ… Stored: datasets/balanced/propensities\\grand_chamber_with_propensity_split_6.pkl\n"
     ]
    }
   ],
   "source": [
    "for split_id in range(0, 7):\n",
    "    compute_and_save_propensities(split_id=split_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3bb7e-3879-4fc6-96cb-b70c52b1735a",
   "metadata": {},
   "source": [
    "### Calculate and store nearest_neightbor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef98c3cb-cf18-44f5-828d-462540c23863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def impute_nn_labels_and_store(split_ids, input_dir='datasets/embeddings', label_col='violation', new_col='label_nn'):\n",
    "    for split_id in tqdm(split_ids, desc=\"Imputing NN labels\"):\n",
    "        chamber_path = os.path.join(input_dir, f'chamber_split_{split_id}.pkl')\n",
    "        grand_path = os.path.join(input_dir, f'grand_chamber_split_{split_id}.pkl')\n",
    "\n",
    "        chamber_df = pd.read_pickle(chamber_path)\n",
    "        grand_df = pd.read_pickle(grand_path)\n",
    "\n",
    "        chamber_emb = torch.stack([torch.from_numpy(x) if isinstance(x, np.ndarray) else torch.tensor(x) for x in chamber_df['embedding']])\n",
    "        grand_emb = torch.stack([torch.from_numpy(x) if isinstance(x, np.ndarray) else torch.tensor(x) for x in grand_df['embedding']])\n",
    "\n",
    "        chamber_norm = torch.nn.functional.normalize(chamber_emb, dim=1)\n",
    "        grand_norm = torch.nn.functional.normalize(grand_emb, dim=1)\n",
    "\n",
    "        sim_matrix = torch.matmul(chamber_norm, grand_norm.T)\n",
    "        nn_indices = torch.argmax(sim_matrix, dim=1).tolist()\n",
    "\n",
    "        imputed_labels = [grand_df.iloc[i][label_col] for i in nn_indices]\n",
    "        chamber_df[new_col] = imputed_labels\n",
    "\n",
    "        out_path = f'datasets/nearest_neighbor/chamber_split_{split_id}.pkl'\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        chamber_df.to_pickle(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff12e85-a087-4e1f-bc8a-07de74983d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Imputing NN labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  8.98it/s]\n"
     ]
    }
   ],
   "source": [
    "impute_nn_labels_and_store(split_ids=range(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9ace6-f27f-4c5e-a630-5f4926b02697",
   "metadata": {},
   "source": [
    "## Extract the expert labels (dissenting opinions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6e1d145a-5817-4103-9598-cd5ed5a1519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from personal_key import my_key # Requires OpenAI key\n",
    "\n",
    "client = OpenAI(api_key=my_key)\n",
    "\n",
    "def make_gpt_call(prompt, model=\"gpt-4.1-mini\"):\n",
    "    return client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "def get_votes(judgment_info, label):\n",
    "    if not isinstance(judgment_info, str): return None\n",
    "    prompt = '''\n",
    "    Read the following ending of a court case file and determine how many judges voted for the violation of article 6. Only output the number of judges who voted for. We only care about article 6, ignore the other articles. If there was a unanimous vote for the violation of article 6, that means 7 judges voted for, since there are 7 judges in the panel. Conversely, if there was a unanimous vote against the violation of article 6, that means 0 judges voted for. \n",
    "\n",
    "    Final decision: {label} (0=no violation, 1=violation)\n",
    "    Text: \n",
    "    {text}\n",
    "\n",
    "    How many judges voted for the violation of article 6? Output a single number only. \n",
    "    '''.format(label=label, text=judgment_info)\n",
    "    return make_gpt_call(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "933be223-0926-49bb-9fbe-8dd0a6b27f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5031it [36:42,  2.28it/s]\n",
      "C:\\Users\\cor\\AppData\\Local\\Temp\\ipykernel_5544\\1326322954.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chamber_df_train['votes_for'] = votes\n"
     ]
    }
   ],
   "source": [
    "votes = []\n",
    "for row_id, row in tqdm(chamber_df_train.iterrows()):\n",
    "    judgment_info = row['judgment_info']\n",
    "    label = row['violation']\n",
    "    votes.append(get_votes(judgment_info, label))\n",
    "chamber_df_train['votes_for'] = votes\n",
    "balanced_sets_chamber = generate_balanced_subsets(chamber_df_train, n=7, random_seed=42)\n",
    "for split_id, c_df in enumerate(balanced_sets_chamber):\n",
    "    c_df.to_csv(f'datasets/votes/chamber_split_{split_id}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc778c76-ec89-46eb-921a-d68cd8d1b54d",
   "metadata": {},
   "source": [
    "#### Investigate token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e221811a-4e49-43ee-bb57-8598bfa038cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "df = chamber_df_train\n",
    "\n",
    "token_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in df['text']]\n",
    "\n",
    "max_length = max(token_lengths)\n",
    "mean_length = np.mean(token_lengths)\n",
    "median_length = np.median(token_lengths)\n",
    "\n",
    "print(f\"Max token length: {max_length}\")\n",
    "print(f\"Mean token length: {mean_length:.1f}\")\n",
    "print(f\"Median token length: {median_length}\")\n",
    "\n",
    "thresholds = [512, 1024, 2048, 4096]\n",
    "for t in thresholds:\n",
    "    pct = sum(l <= t for l in token_lengths) / len(token_lengths) * 100\n",
    "    print(f\"Percentage of samples with â‰¤ {t} tokens: {pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c071c3a1-6d2c-46ec-a517-08cdf7589c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "===== Chamber =====\n",
      "Total cases: 5031\n",
      "\n",
      "-- Raw --\n",
      "Max token length: 75456\n",
      "Mean token length: 2317.9\n",
      "Median token length: 1303.0\n",
      "â‰¤ 512 tokens: 20.8%\n",
      "â‰¤ 1024 tokens: 41.6%\n",
      "â‰¤ 2048 tokens: 65.4%\n",
      "â‰¤ 4096 tokens: 85.3%\n",
      "\n",
      "-- Preprocessed (Cleaned + Head-Tail) --\n",
      "Max token length: 4096\n",
      "Mean token length: 1756.0\n",
      "Median token length: 1303.0\n",
      "â‰¤ 512 tokens: 20.8%\n",
      "â‰¤ 1024 tokens: 41.6%\n",
      "â‰¤ 2048 tokens: 65.4%\n",
      "â‰¤ 4096 tokens: 100.0%\n",
      "\n",
      "test\n",
      "===== Grand Chamber =====\n",
      "Total cases: 157\n",
      "\n",
      "-- Raw --\n",
      "Max token length: 22029\n",
      "Mean token length: 5638.6\n",
      "Median token length: 4466.0\n",
      "â‰¤ 512 tokens: 2.5%\n",
      "â‰¤ 1024 tokens: 7.6%\n",
      "â‰¤ 2048 tokens: 17.2%\n",
      "â‰¤ 4096 tokens: 45.9%\n",
      "\n",
      "-- Preprocessed (Cleaned + Head-Tail) --\n",
      "Max token length: 4096\n",
      "Mean token length: 3275.6\n",
      "Median token length: 4096.0\n",
      "â‰¤ 512 tokens: 2.5%\n",
      "â‰¤ 1024 tokens: 7.6%\n",
      "â‰¤ 2048 tokens: 17.2%\n",
      "â‰¤ 4096 tokens: 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def head_tail_truncate_ids(text, tokenizer, max_tokens=4094, head_ratio=0.5):\n",
    "    text = clean_text(text)\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return tokens\n",
    "\n",
    "    head_len = int(max_tokens * head_ratio)\n",
    "    tail_len = max_tokens - head_len\n",
    "    return tokens[:head_len] + tokens[-tail_len:]\n",
    "\n",
    "def print_stats(name, texts):\n",
    "    raw_lengths = [len(tokenizer.encode(t, add_special_tokens=True)) for t in texts]\n",
    "    truncated_ids = [head_tail_truncate_ids(t, tokenizer) for t in texts]\n",
    "    truncated_lengths = [len(ids) + 2 for ids in truncated_ids]  # +2 for special tokens\n",
    "\n",
    "    print(f\"===== {name} =====\")\n",
    "    print(f\"Total cases: {len(texts)}\")\n",
    "\n",
    "    def summarize(lengths, label):\n",
    "        print(f\"\\n-- {label} --\")\n",
    "        print(f\"Max token length: {max(lengths)}\")\n",
    "        print(f\"Mean token length: {np.mean(lengths):.1f}\")\n",
    "        print(f\"Median token length: {np.median(lengths):.1f}\")\n",
    "        for t in [512, 1024, 2048, 4096]:\n",
    "            pct = sum(l <= t for l in lengths) / len(lengths) * 100\n",
    "            print(f\"â‰¤ {t} tokens: {pct:.1f}%\")\n",
    "\n",
    "    summarize(raw_lengths, \"Raw\")\n",
    "    summarize(truncated_lengths, \"Preprocessed (Cleaned + Head-Tail)\")\n",
    "    print()\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "print_stats(\"Chamber\", chamber_df_train['text'])\n",
    "print_stats(\"Grand Chamber\", grand_chamber_df_train['text'])\n",
    "print_stats(\"Both\", pd.concat([chamber_df_train['text'], grand_chamber_df_train['text']]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
